{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioWaKBpaepao"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np, cv2, matplotlib.pyplot as plt\n",
        "model = keras.models.load_model('/content/cvpr_m.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "CX2ej-JrehTy",
        "outputId": "eb38078a-cc36-42f6-8d3d-039c32bf422e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      async function snap(quality, boxW, boxH) {\n",
              "        const root = document.createElement('div');\n",
              "        root.style.position = 'relative';\n",
              "\n",
              "        const btn = document.createElement('button');\n",
              "        btn.textContent = 'Capture';\n",
              "        btn.style.margin = '8px 0';\n",
              "        root.appendChild(btn);\n",
              "\n",
              "        const video = document.createElement('video');\n",
              "        video.style.display = 'block';\n",
              "        root.appendChild(video);\n",
              "\n",
              "        const overlay = document.createElement('canvas');\n",
              "        overlay.style.position = 'absolute';\n",
              "        overlay.style.left = '0';\n",
              "        overlay.style.top = '0';\n",
              "        overlay.style.pointerEvents = 'none';\n",
              "        root.appendChild(overlay);\n",
              "\n",
              "        const stream = await navigator.mediaDevices.getUserMedia({video:true});\n",
              "        document.body.appendChild(root);\n",
              "        video.srcObject = stream;\n",
              "        await video.play();\n",
              "\n",
              "        const vw = () => video.videoWidth, vh = () => video.videoHeight;\n",
              "\n",
              "        const full = document.createElement('canvas');\n",
              "        const fctx = full.getContext('2d');\n",
              "        const crop = document.createElement('canvas');\n",
              "        const cctx = crop.getContext('2d');\n",
              "\n",
              "\n",
              "        let raf;\n",
              "        function drawOverlay(){\n",
              "          if (vw()===0 || vh()===0) { raf=requestAnimationFrame(drawOverlay); return; }\n",
              "          overlay.width = vw(); overlay.height = vh();\n",
              "          const o = overlay.getContext('2d');\n",
              "          o.clearRect(0,0,overlay.width,overlay.height);\n",
              "          const bw = Math.round(vw()*boxW);\n",
              "          const bh = Math.round(vh()*boxH);\n",
              "          const bx = Math.round((vw()-bw)/2);\n",
              "          const by = Math.round((vh()-bh)/2);\n",
              "          o.fillStyle = 'rgba(0,0,0,0.35)'; o.fillRect(0,0,overlay.width,overlay.height);\n",
              "          o.clearRect(bx,by,bw,bh);\n",
              "          o.strokeStyle = '#00FF99'; o.lineWidth = 4; o.strokeRect(bx,by,bw,bh);\n",
              "          raf=requestAnimationFrame(drawOverlay);\n",
              "        }\n",
              "        drawOverlay();\n",
              "\n",
              "        // wait for click\n",
              "        await new Promise(resolve => btn.onclick = resolve);\n",
              "\n",
              "        // capture and crop\n",
              "        full.width = vw(); full.height = vh();\n",
              "        fctx.drawImage(video, 0, 0);\n",
              "        const bw = Math.round(vw()*boxW);\n",
              "        const bh = Math.round(vh()*boxH);\n",
              "        const bx = Math.round((vw()-bw)/2);\n",
              "        const by = Math.round((vh()-bh)/2);\n",
              "        crop.width = bw; crop.height = bh;\n",
              "        cctx.drawImage(video, bx, by, bw, bh, 0, 0, bw, bh);\n",
              "        const dataUrl = crop.toDataURL('image/jpeg', quality);\n",
              "\n",
              "        cancelAnimationFrame(raf);\n",
              "        stream.getVideoTracks()[0].stop();\n",
              "        root.remove();\n",
              "        return dataUrl;\n",
              "      }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2636213748.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_guided_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_w_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_h_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_like_boxonly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2636213748.py\u001b[0m in \u001b[0;36mtake_guided_photo\u001b[0;34m(quality, box_w_frac, box_h_frac)\u001b[0m\n\u001b[1;32m     79\u001b[0m     ''')\n\u001b[1;32m     80\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mdata_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'snap({quality}, {box_w_frac}, {box_h_frac})'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np, cv2, matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "\n",
        "def take_guided_photo(quality=0.95, box_w_frac=0.6, box_h_frac=0.6):\n",
        "    js = Javascript(r'''\n",
        "      async function snap(quality, boxW, boxH) {\n",
        "        const root = document.createElement('div');\n",
        "        root.style.position = 'relative';\n",
        "\n",
        "        const btn = document.createElement('button');\n",
        "        btn.textContent = 'Capture';\n",
        "        btn.style.margin = '8px 0';\n",
        "        root.appendChild(btn);\n",
        "\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        root.appendChild(video);\n",
        "\n",
        "        const overlay = document.createElement('canvas');\n",
        "        overlay.style.position = 'absolute';\n",
        "        overlay.style.left = '0';\n",
        "        overlay.style.top = '0';\n",
        "        overlay.style.pointerEvents = 'none';\n",
        "        root.appendChild(overlay);\n",
        "\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video:true});\n",
        "        document.body.appendChild(root);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        const vw = () => video.videoWidth, vh = () => video.videoHeight;\n",
        "\n",
        "        const full = document.createElement('canvas');\n",
        "        const fctx = full.getContext('2d');\n",
        "        const crop = document.createElement('canvas');\n",
        "        const cctx = crop.getContext('2d');\n",
        "\n",
        "\n",
        "        let raf;\n",
        "        function drawOverlay(){\n",
        "          if (vw()===0 || vh()===0) { raf=requestAnimationFrame(drawOverlay); return; }\n",
        "          overlay.width = vw(); overlay.height = vh();\n",
        "          const o = overlay.getContext('2d');\n",
        "          o.clearRect(0,0,overlay.width,overlay.height);\n",
        "          const bw = Math.round(vw()*boxW);\n",
        "          const bh = Math.round(vh()*boxH);\n",
        "          const bx = Math.round((vw()-bw)/2);\n",
        "          const by = Math.round((vh()-bh)/2);\n",
        "          o.fillStyle = 'rgba(0,0,0,0.35)'; o.fillRect(0,0,overlay.width,overlay.height);\n",
        "          o.clearRect(bx,by,bw,bh);\n",
        "          o.strokeStyle = '#00FF99'; o.lineWidth = 4; o.strokeRect(bx,by,bw,bh);\n",
        "          raf=requestAnimationFrame(drawOverlay);\n",
        "        }\n",
        "        drawOverlay();\n",
        "\n",
        "        // wait for click\n",
        "        await new Promise(resolve => btn.onclick = resolve);\n",
        "\n",
        "        // capture and crop\n",
        "        full.width = vw(); full.height = vh();\n",
        "        fctx.drawImage(video, 0, 0);\n",
        "        const bw = Math.round(vw()*boxW);\n",
        "        const bh = Math.round(vh()*boxH);\n",
        "        const bx = Math.round((vw()-bw)/2);\n",
        "        const by = Math.round((vh()-bh)/2);\n",
        "        crop.width = bw; crop.height = bh;\n",
        "        cctx.drawImage(video, bx, by, bw, bh, 0, 0, bw, bh);\n",
        "        const dataUrl = crop.toDataURL('image/jpeg', quality);\n",
        "\n",
        "        cancelAnimationFrame(raf);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        root.remove();\n",
        "        return dataUrl;\n",
        "      }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data_url = eval_js(f'snap({quality}, {box_w_frac}, {box_h_frac})')\n",
        "    b = b64decode(data_url.split(',')[1])\n",
        "    arr = np.frombuffer(b, np.uint8)\n",
        "    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)  # BGR crop of the box\n",
        "    if img is None: raise RuntimeError(\"Capture failed\")\n",
        "    return img\n",
        "\n",
        "\n",
        "def mnist_like_boxonly(box_bgr):\n",
        "    g = cv2.cvtColor(box_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    H,W = g.shape\n",
        "    pad = int(0.10*min(H,W))\n",
        "    g = g[pad:H-pad, pad:W-pad]  # trim border\n",
        "    if g.size == 0: return None, None\n",
        "\n",
        "    bg = cv2.GaussianBlur(g,(0,0),21)\n",
        "    norm = cv2.normalize(g-bg,None,0,255,cv2.NORM_MINMAX)\n",
        "\n",
        "    th1 = cv2.adaptiveThreshold(norm,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                cv2.THRESH_BINARY,31,5)\n",
        "    _, th2 = cv2.threshold(cv2.GaussianBlur(norm,(5,5),0),0,255,\n",
        "                           cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    th = th1 if (th1==255).sum() < (th2==255).sum()*1.2 else th2\n",
        "    if th.mean()>127: th = 255-th\n",
        "\n",
        "    th = cv2.morphologyEx(th, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
        "    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, np.ones((3,3),np.uint8))\n",
        "\n",
        "    cnts,_ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not cnts: return None, None\n",
        "    A=th.shape[0]*th.shape[1]; cx,cy=W/2,H/2\n",
        "    best=None; score_best=-1e18\n",
        "    for c in cnts:\n",
        "        x,y,w,h=cv2.boundingRect(c); area=w*h\n",
        "        if area<0.002*A or area>0.25*A: continue\n",
        "        ar=w/max(h,1)\n",
        "        if not (0.4<=ar<=2.5): continue\n",
        "        mx,my=x+w/2,y+h/2; d2=(mx-cx)**2+(my-cy)**2\n",
        "        score=(area/A)-0.002*d2\n",
        "        if score>score_best: score_best=score; best=(x,y,w,h)\n",
        "    if best is None: return None, None\n",
        "    x,y,w,h=best; roi=th[y:y+h,x:x+w]\n",
        "\n",
        "    if h>w: new_h,new_w=20,int(round(w*20/h))\n",
        "    else:   new_w,new_h=20,int(round(h*20/w))\n",
        "    small=cv2.resize(roi,(new_w,new_h),interpolation=cv2.INTER_AREA)\n",
        "    top=(28-new_h)//2; bottom=28-new_h-top\n",
        "    left=(28-new_w)//2; right=28-new_w-left\n",
        "    canvas=cv2.copyMakeBorder(small,top,bottom,left,right,cv2.BORDER_CONSTANT,value=0)\n",
        "\n",
        "    ys,xs=np.nonzero(canvas)\n",
        "    if len(xs) and len(ys):\n",
        "        cxo,cyo=xs.mean(),ys.mean()\n",
        "        M=np.float32([[1,0,14-cxo],[0,1,14-cyo]])\n",
        "        canvas=cv2.warpAffine(canvas,M,(28,28),borderValue=0)\n",
        "\n",
        "    x28=(canvas.astype(\"float32\")/255.0)[None,...,None]  # (1,28,28,1)\n",
        "    return x28, canvas\n",
        "\n",
        "\n",
        "crop = take_guided_photo(quality=0.95, box_w_frac=0.6, box_h_frac=0.6)\n",
        "x, vis = mnist_like_boxonly(crop)\n",
        "if x is None:\n",
        "    raise RuntimeError(\"No digit found in the guided box.\")\n",
        "\n",
        "probs = model.predict(x, verbose=0)[0]\n",
        "pred, conf = int(np.argmax(probs)), float(np.max(probs))\n",
        "print(\"Predicted:\", pred, \"Confidence:\", conf)\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1); plt.imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)); plt.title(\"Captured box\"); plt.axis(\"off\")\n",
        "plt.subplot(1,2,2); plt.imshow(vis, cmap='gray'); plt.title(f\"MNIST 28×28\\npred={pred} conf={conf:.3f}\"); plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}